{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Football Diffusion: Colab Runner\n",
    "\n",
    "This notebook assumes you uploaded the entire repo to Colab (e.g., via a zip) and have raw NFL Big Data Bowl CSVs accessible (either in Google Drive or alongside the repo). It mounts Drive, installs deps, sets `PYTHONPATH`, and lets you preprocess, train, and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "#@title Mount Google Drive (optional)\n",
    "USE_DRIVE = True  #@param {type:\"boolean\"}\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_DIR = '/content/drive/MyDrive/dl_project'  # change if you put the repo elsewhere in Drive\n",
    "else:\n",
    "    BASE_DIR = '/content/dl_project'  # update if you unzip elsewhere\n",
    "\n",
    "%cd $BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "#@title Install dependencies\n",
    "!pip install --quiet torch pytorch-lightning pandas numpy pyarrow tqdm scikit-learn tabulate pyyaml matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "env-setup"
   },
   "outputs": [],
   "source": [
    "#@title Set paths and PYTHONPATH\n",
    "import os, sys, pathlib\n",
    "\n",
    "repo_root = pathlib.Path(BASE_DIR)\n",
    "diffusion_dir = repo_root / 'diffusion'\n",
    "data_raw = repo_root / 'data' / 'nfl-big-data-bowl-2023'  # adjust if stored elsewhere\n",
    "data_cache = repo_root / 'data' / 'cache'\n",
    "artifacts_dir = repo_root / 'artifacts' / 'diffusion'\n",
    "config_train = diffusion_dir / 'src' / 'football_diffusion' / 'config' / 'train.yaml'\n",
    "config_eval = diffusion_dir / 'src' / 'football_diffusion' / 'config' / 'eval.yaml'\n",
    "\n",
    "os.environ['PYTHONPATH'] = str(diffusion_dir / 'src') + ':' + os.environ.get('PYTHONPATH', '')\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'  # harmless on Colab\n",
    "\n",
    "print('Repo root:', repo_root)\n",
    "print('Raw data:', data_raw)\n",
    "print('Cache dir:', data_cache)\n",
    "print('Artifacts:', artifacts_dir)\n",
    "print('PYTHONPATH:', os.environ['PYTHONPATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Preprocess data\n",
    "- Requires CSVs: `plays.csv`, `games.csv`, `players.csv`, `week*.csv` in `data/nfl-big-data-bowl-2023/` (or adjust `data_raw`).\n",
    "- Generates `processed_plays.pkl` and `metadata.json` in `data/cache/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess"
   },
   "outputs": [],
   "source": [
    "#@title Run preprocessing (skip if cache already built)\n",
    "from pathlib import Path\n",
    "\n",
    "data_cache.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "!bash $diffusion_dir/scripts/preprocess.sh \\\n",
    "  $data_raw \\\n",
    "  $data_cache \\\n",
    "  $diffusion_dir/src/football_diffusion/config/default.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Train diffusion model (optional)\n",
    "- Uses cached data; writes checkpoints to `artifacts/diffusion/`.\n",
    "- Set `MAX_EPOCHS` lower if you just want a smoke test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "#@title Train (set MAX_EPOCHS as needed)\n",
    "MAX_EPOCHS = 5  #@param {type:\"integer\"}\n",
    "\n",
    "!python $diffusion_dir/train_main.py \\\n",
    "  --config $config_train \\\n",
    "  --cache_dir $data_cache \\\n",
    "  --output_dir $artifacts_dir \\\n",
    "  --devices 1 \\\n",
    "  --max_epochs $MAX_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Evaluate\n",
    "- Point `CHECKPOINT` to your saved `.ckpt` (e.g., `last.ckpt`).\n",
    "- Uses sample steps 20/50/100 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d1b59",
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "#@title Evaluate checkpoint\n",
    "import glob, os, json, sys, torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Resolve diffusion_dir robustly\n",
    "if \"diffusion_dir\" not in globals():\n",
    "    candidates = [Path.cwd(), Path.cwd() / \"diffusion\", Path(\"/content/dl_project/diffusion\"), Path(\"/content/drive/MyDrive/dl_project/diffusion\")]\n",
    "    diffusion_dir = None\n",
    "    for c in candidates:\n",
    "        if (c / \"src\" / \"football_diffusion\").exists():\n",
    "            diffusion_dir = c.resolve()\n",
    "            break\n",
    "    if diffusion_dir is None:\n",
    "        raise RuntimeError(\"Could not locate diffusion directory; set diffusion_dir manually.\")\n",
    "\n",
    "os.environ['PYTHONPATH'] = str(diffusion_dir / 'src') + ':' + os.environ.get('PYTHONPATH', '')\n",
    "sys.path.insert(0, str(diffusion_dir / 'src'))\n",
    "\n",
    "DEFAULT_CKPT = sorted(glob.glob(str((diffusion_dir.parent / 'artifacts' / 'diffusion' / '*.ckpt')))) if 'artifacts_dir' not in globals() else sorted(glob.glob(str(artifacts_dir / '*.ckpt')))\n",
    "CHECKPOINT = DEFAULT_CKPT[-1] if DEFAULT_CKPT else ''  #@param {type:\"string\"}\n",
    "\n",
    "if not CHECKPOINT:\n",
    "    raise ValueError(\"No checkpoint found; train first or set CHECKPOINT manually\")\n",
    "\n",
    "from football_diffusion.eval.eval_diffusion import run_evaluation\n",
    "\n",
    "device_pref = 'cuda' if torch.cuda.is_available() else ('mps' if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "cache_dir = diffusion_dir.parent / 'data' / 'cache' if 'data_cache' not in globals() else data_cache\n",
    "config_eval_path = diffusion_dir / 'src' / 'football_diffusion' / 'config' / 'eval.yaml' if 'config_eval' not in globals() else config_eval\n",
    "\n",
    "results = run_evaluation(\n",
    "    checkpoint_path=str(CHECKPOINT),\n",
    "    cache_dir=str(cache_dir),\n",
    "    config_path=str(config_eval_path),\n",
    "    split='test',\n",
    "    batch_size=8,\n",
    "    num_samples=8,\n",
    "    sample_steps=[50, 100],\n",
    "    ddim=True,\n",
    "    device=device_pref\n",
    ")\n",
    "print('Using device:', device_pref)\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900769a4",
   "metadata": {},
   "source": [
    "## 4) Visualize trajectories\n",
    "- Animate a ground-truth play and a generated sample.\n",
    "- Uses cached data and the chosen checkpoint; denormalizes before plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f44538",
   "metadata": {
    "id": "visualize"
   },
   "outputs": [],
   "source": [
    "#@title Animate a sample play (ground truth vs generated)\n",
    "import os, sys, yaml, torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Resolve diffusion_dir robustly\n",
    "if \"diffusion_dir\" not in globals():\n",
    "    candidates = [Path.cwd(), Path.cwd() / \"diffusion\", Path(\"/content/dl_project/diffusion\"), Path(\"/content/drive/MyDrive/dl_project/diffusion\")]\n",
    "    diffusion_dir = None\n",
    "    for c in candidates:\n",
    "        if (c / \"src\" / \"football_diffusion\").exists():\n",
    "            diffusion_dir = c.resolve()\n",
    "            break\n",
    "    if diffusion_dir is None:\n",
    "        raise RuntimeError(\"Could not locate diffusion directory; set diffusion_dir manually.\")\n",
    "\n",
    "cache_dir = diffusion_dir.parent / \"data\" / \"cache\" if \"data_cache\" not in globals() else data_cache\n",
    "config_train_path = diffusion_dir / \"src\" / \"football_diffusion\" / \"config\" / \"train.yaml\" if \"config_train\" not in globals() else config_train\n",
    "\n",
    "os.environ['PYTHONPATH'] = str(diffusion_dir / 'src') + ':' + os.environ.get('PYTHONPATH', '')\n",
    "sys.path.insert(0, str(diffusion_dir / \"src\"))\n",
    "from football_diffusion.data.dataset import FootballPlayDataset\n",
    "from football_diffusion.training.train_diffusion import DiffusionLightningModule\n",
    "from football_diffusion.viz.animate import animate_comparison\n",
    "\n",
    "if \"CHECKPOINT\" not in globals() or not CHECKPOINT:\n",
    "    raise ValueError(\"Set CHECKPOINT above (run the evaluate cell)\")\n",
    "\n",
    "cfg = yaml.safe_load(open(config_train_path))\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "ds = FootballPlayDataset(cache_dir / \"processed_plays.pkl\", cache_dir / \"metadata.json\", split=\"val\")\n",
    "if len(ds) == 0:\n",
    "    raise ValueError(\"Dataset is empty; ensure preprocessing ran correctly.\")\n",
    "SAMPLE_IDX = 0  # change to visualize a different play\n",
    "sample = ds[SAMPLE_IDX]\n",
    "context_cat = [sample[\"context_categorical\"]]\n",
    "context_cont = sample[\"context_continuous\"].unsqueeze(0).to(device)\n",
    "\n",
    "module = DiffusionLightningModule.load_from_checkpoint(str(CHECKPOINT), config=cfg)\n",
    "model = module.model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    gen = model.sample(\n",
    "        shape=sample[\"X\"].shape[0:3],\n",
    "        context_categorical=context_cat,\n",
    "        context_continuous=context_cont,\n",
    "        num_steps=100,\n",
    "        ddim=True,\n",
    "        smooth=True,\n",
    "    )\n",
    "\n",
    "gt_np = ds.denormalize_tensor(sample[\"X\"].numpy())  # [T, P, F]\n",
    "gen_np = ds.denormalize_tensor(gen.cpu().numpy()[0])  # [T, P, F]\n",
    "\n",
    "gt_xy = gt_np[:, :, :2]\n",
    "gen_xy = gen_np[:, :, :2]\n",
    "\n",
    "anim, fig = animate_comparison([gt_xy, gen_xy], labels=[\"Ground Truth\", \"Generated\"], interval=120)\n",
    "HTML(anim.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4c0e4",
   "metadata": {},
   "source": [
    "## 5) Load and inspect outputs\n",
    "- `eval_results.json` is written next to the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inspect-results"
   },
   "outputs": [],
   "source": [
    "#@title Show saved eval results (if present)\n",
    "import json, pathlib\n",
    "\n",
    "results_file = pathlib.Path(CHECKPOINT).parent / 'eval_results.json'\n",
    "if results_file.exists():\n",
    "    print(json.dumps(json.load(open(results_file)), indent=2))\n",
    "else:\n",
    "    print('No eval_results.json found; run evaluation first.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}